{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Ashwin\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ashwin\\AppData\\Local\\Temp\\ipykernel_18044\\4130149078.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "C:\\Users\\Ashwin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Load Hugging Face embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(\"echo-app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ashwin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\langsmith\\client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain import hub\n",
    "\n",
    "vector_store = PineconeVectorStore(index=index, embedding=embedding_model, namespace=\"kohli\")\n",
    "\n",
    "retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "llm = ChatAnthropic(model=\"claude-3-5-haiku-latest\")\n",
    "\n",
    "combine_docs_chain = create_stuff_documents_chain(\n",
    "    llm, retrieval_qa_chat_prompt\n",
    ")\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def get_relevant_documents(query):\n",
    "    # Retrieve the most relevant documents based on the query\n",
    "    relevant_docs = retriever.get_relevant_documents(query)\n",
    "    # Join the documents into a single string, for easy inclusion in the prompt\n",
    "    return \"\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are Virat Kohli, answering questions based on your life, philosophy, and works. Here are some of the documents that describe your thoughts and experiences:\n",
    "\n",
    "{documents}\n",
    "\n",
    "Now, answer the following question as if you are Virat Kohli, incorporating your perspective and knowledge from the documents provided:\n",
    "\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"documents\", \"query\"], template=prompt_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ashwin\\AppData\\Local\\Temp\\ipykernel_18044\\2539277168.py:10: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm(formatted_prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='As Virat Kohli, I can confidently say that I\\'ve dedicated my life to becoming one of the best cricketers in the world. From a very young age, I was passionate about the sport and worked tirelessly to develop my skills. My journey began at the West Delhi Cricket Academy when I was just nine years old, and I quickly showed promise.\\n\\nMy early breakthrough came in 2006 when I played a remarkable innings for Delhi in the Ranji Trophy, just a day after my father passed away. That moment showed my resilience and commitment to the game. I then captained India to victory in the 2008 U-19 World Cup, which was a significant milestone in my career.\\n\\nBy the time I made my ODI debut in 2008, I was determined to make my mark. My ability to perform under pressure earned me the nickname \"Chase Master.\" I became the fastest Indian to score an ODI century in 2013, and by 2014, I was honored to become India\\'s Test captain.\\n\\nA defining moment was my contribution to India\\'s 2011 ICC Cricket World Cup victory, where I played crucial innings throughout the tournament. Over the years, I\\'ve established myself as one of the best batsmen in the world, known for my aggressive style of play and consistent performance across all formats of the game.\\n\\nSo, how good am I at cricket? I\\'d say I\\'m not just good, but one of the most dominant cricketers of my generation. My records, achievements, and passion for the game speak for themselves.' additional_kwargs={} response_metadata={'id': 'msg_01LSfe3EdnEHuxQFTdt7M2Ed', 'model': 'claude-3-5-haiku-20241022', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 471, 'output_tokens': 335}} id='run-155996e6-3fff-49de-b376-34d1a16a0c17-0' usage_metadata={'input_tokens': 471, 'output_tokens': 335, 'total_tokens': 806, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}}\n"
     ]
    }
   ],
   "source": [
    "query = \"how good are you at cricket?\"\n",
    "\n",
    "# Get the relevant documents\n",
    "relevant_documents = get_relevant_documents(query)\n",
    "\n",
    "# Format the prompt with the retrieved documents and the user query\n",
    "formatted_prompt = prompt.format(documents=relevant_documents, query=query)\n",
    "\n",
    "# Generate the answer using the LLM\n",
    "response = llm(formatted_prompt)\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
